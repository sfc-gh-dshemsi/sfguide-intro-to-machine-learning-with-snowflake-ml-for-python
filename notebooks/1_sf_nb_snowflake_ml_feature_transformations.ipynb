{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "lastEditStatus": {
      "notebookId": "l42viwe4q5ln52hxupdl",
      "authorId": "158808794318",
      "authorName": "SIKHADAS",
      "authorEmail": "sikha.das@snowflake.com",
      "sessionId": "688caa76-5ca5-4722-a606-87d3ca113c57",
      "lastEditTime": 1747163540467
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c0f7cf92-65a9-48b2-9212-d8807ba454bc",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "resultHeight": 227,
        "codeCollapsed": true
      },
      "source": "## 2. ML Feature Transformations\n\n- In this notebook, we will walk through a few transformations that are included in the Snowflake ML Preprocessing API. \n- We will also build a preprocessing pipeline to be used in the ML modeling notebook.\n\n***Note: All feature transformations using Snowflake ML are distributed operations in the same way that Snowpark DataFrame operations are.***"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "96992c21-d30a-400e-a034-0bfd96a1200f",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "resultHeight": 46,
        "codeCollapsed": true
      },
      "source": "### Import Libraries"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fd506e2-20a7-43fd-9630-7fc0468c9652",
      "metadata": {
        "language": "python",
        "resultHeight": 0
      },
      "outputs": [],
      "source": "# Snowpark for Python\nimport snowflake.snowpark.functions as F\nfrom snowflake.snowpark.types import DecimalType\nfrom snowflake.snowpark.context import get_active_session\n\n# Snowflake ML\nimport snowflake.ml.modeling.preprocessing as snowml\nfrom snowflake.ml.modeling.pipeline import Pipeline\nfrom snowflake.ml.modeling.metrics.correlation import correlation\n\n# Data Science Libs\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Misc\nimport json\nimport joblib\n\n# warning suppresion\nimport warnings; warnings.simplefilter('ignore')"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "69454482-73d4-4d81-90ad-374152e5bcb9",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "resultHeight": 113,
        "codeCollapsed": true
      },
      "source": "### Establish Secure Connection to Snowflake\n\nNotebooks establish a Snowpark Session when the notebook is attached to the kernel. Let's use that Session object to validate our connection."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fec397de-9822-4e60-aabe-969a630849f5",
      "metadata": {
        "language": "sql",
        "resultHeight": 112,
        "resultVariableName": "init_sql"
      },
      "outputs": [],
      "source": "-- Using Warehouse, Database, and Schema created during Setup\nUSE WAREHOUSE ML_HOL_WH;\nUSE DATABASE ML_HOL_DB;\nUSE SCHEMA ML_HOL_SCHEMA;"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6aeaefb-b580-47d3-8382-e915e33b1d4c",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "resultHeight": 84
      },
      "outputs": [],
      "source": "session = get_active_session()\n\n# Set the database and schema context\nsession.use_warehouse(\"ML_HOL_WH\")\nsession.use_database(\"ML_HOL_DB\")\nsession.use_schema(\"ML_HOL_SCHEMA\")\n\n# Add a query tag to the session.\nsession.query_tag = {\"origin\":\"sf_sit-is\", \n                     \"name\":\"e2e_ml_snowparkpython\", \n                     \"version\":{\"major\":1, \"minor\":0,},\n                     \"attributes\":{\"is_quickstart\":1}}\nsession"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "71ac9d3f-0e4e-466c-b500-868ab63b69e9",
      "metadata": {
        "resultHeight": 46,
        "codeCollapsed": true
      },
      "source": "### Data Loading"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29aa8b77-5ab8-4953-b7b9-2c023a1d03cc",
      "metadata": {
        "language": "python",
        "resultHeight": 0
      },
      "outputs": [],
      "source": "# First, we read in the data from a Snowflake table into a Snowpark DataFrame\n# **Change this only if you named your table something else in the data ingest notebook **\n# Load in the data\n# Using fully qualified table name to ensure we're reading from the correct schema\ndiamonds_df = session.table(\"ML_HOL_DB.ML_HOL_SCHEMA.DIAMONDS\")\n"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "7a052fa9-922e-4d72-a41d-f1795bfac6ea",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "resultHeight": 113,
        "codeCollapsed": true
      },
      "source": "### Feature Transformations\n\nWe will illustrate a few of the transformation functions here, but the rest can be found in the [documentation](https://docs.snowflake.com/LIMITEDACCESS/snowflake-ml-preprocessing)."
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "265507af-8e9c-4d8a-987d-914d249309c2",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "resultHeight": 31,
        "codeCollapsed": true
      },
      "source": "##### Let's use the `MinMaxScaler` to normalize the `CARAT` column."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00c962dc-1724-4eec-a3b6-bd255e9df4bf",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "resultHeight": 439
      },
      "outputs": [],
      "source": "# Normalize the CARAT column\nsnowml_mms = snowml.MinMaxScaler(input_cols=[\"CARAT\"], output_cols=[\"CARAT_NORM\"])\nnormalized_diamonds_df = snowml_mms.fit(diamonds_df).transform(diamonds_df)\n\n# Reduce the number of decimals\nnew_col = normalized_diamonds_df.col(\"CARAT_NORM\").cast(DecimalType(7, 6))\nnormalized_diamonds_df = normalized_diamonds_df.with_column(\"CARAT_NORM\", new_col)\n\nnormalized_diamonds_df"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3d5292e4-613f-4f4c-9579-aa338cf0f6eb",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "resultHeight": 47,
        "codeCollapsed": true
      },
      "source": "##### Let's use the `OrdinalEncoder` to transform `COLOR` and `CLARITY` from categorical to numerical values so they are more meaningful."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cf97705-c30b-48e9-b152-76e0ccc3a818",
      "metadata": {
        "language": "python",
        "resultHeight": 797
      },
      "outputs": [],
      "source": "# Encode CUT and CLARITY preserve ordinal importance\ncategories = {\n    \"CUT\": np.array([\"IDEAL\", \"PREMIUM\", \"VERY_GOOD\", \"GOOD\", \"FAIR\"]),\n    \"CLARITY\": np.array([\"IF\", \"VVS1\", \"VVS2\", \"VS1\", \"VS2\", \"SI1\", \"SI2\", \"I1\", \"I2\", \"I3\"]),\n}\nsnowml_oe = snowml.OrdinalEncoder(input_cols=[\"CUT\", \"CLARITY\"], output_cols=[\"CUT_OE\", \"CLARITY_OE\"], categories=categories)\nord_encoded_diamonds_df = snowml_oe.fit(normalized_diamonds_df).transform(normalized_diamonds_df)\n\n# Show the encoding\nprint(snowml_oe._state_pandas)\n\nord_encoded_diamonds_df"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "412bfa35-f9dd-459a-99b3-5dd5d846f9e6",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "resultHeight": 166,
        "codeCollapsed": true
      },
      "source": "##### Let's use the `OneHotEncoder` to transform the categorical columns to numerical columns.\n\nThis is more for illustration purposes. Using the OrdinalEncoder makes more sense for the diamonds dataset since `CARAT`, `COLOR`, and `CLARITY` all follow a natural ranking order."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95781b03-1996-4171-8ebd-0a41f676a359",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "resultHeight": 439
      },
      "outputs": [],
      "source": "# Encode categoricals to numeric columns\nsnowml_ohe = snowml.OneHotEncoder(input_cols=[\"CUT\", \"COLOR\", \"CLARITY\"], output_cols=[\"CUT_OHE\", \"COLOR_OHE\", \"CLARITY_OHE\"])\ntransformed_diamonds_df = snowml_ohe.fit(ord_encoded_diamonds_df).transform(ord_encoded_diamonds_df)\n\nnp.array(transformed_diamonds_df.columns)"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "950944f6-de76-4820-a185-18feb4b24462",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "resultHeight": 99,
        "codeCollapsed": true
      },
      "source": "##### Finally, we can also build out a full preprocessing `Pipeline`.\n\nThis will be useful for both the ML training & inference steps to have standarized feature transformations."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1cd00f7-46ac-46d4-804e-2894b01a1197",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "resultHeight": 0
      },
      "outputs": [],
      "source": "# Categorize all the features for processing\nCATEGORICAL_COLUMNS = [\"CUT\", \"COLOR\", \"CLARITY\"]\nCATEGORICAL_COLUMNS_OE = [\"CUT_OE\", \"COLOR_OE\", \"CLARITY_OE\"] # To name the ordinal encoded columns\nNUMERICAL_COLUMNS = [\"CARAT\", \"DEPTH\", \"TABLE_PCT\", \"X\", \"Y\", \"Z\"]\n\ncategories = {\n    \"CUT\": np.array([\"IDEAL\", \"PREMIUM\", \"VERY_GOOD\", \"GOOD\", \"FAIR\"]),\n    \"CLARITY\": np.array([\"IF\", \"VVS1\", \"VVS2\", \"VS1\", \"VS2\", \"SI1\", \"SI2\", \"I1\", \"I2\", \"I3\"]),\n    \"COLOR\": np.array(['D', 'E', 'F', 'G', 'H', 'I', 'J']),\n}"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4082f2e2-a2ae-4858-9e4c-a0fd1540dd4d",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "resultHeight": 439
      },
      "outputs": [],
      "source": "# Build the pipeline\npreprocessing_pipeline = Pipeline(\n    steps=[\n            (\n                \"OE\",\n                snowml.OrdinalEncoder(\n                    input_cols=CATEGORICAL_COLUMNS,\n                    output_cols=CATEGORICAL_COLUMNS_OE,\n                    categories=categories,\n                )\n            ),\n            (\n                \"MMS\",\n                snowml.MinMaxScaler(\n                    clip=True,\n                    input_cols=NUMERICAL_COLUMNS,\n                    output_cols=NUMERICAL_COLUMNS,\n                )\n            )\n    ]\n)\n\nPIPELINE_FILE = '/tmp/preprocessing_pipeline.joblib'\njoblib.dump(preprocessing_pipeline, PIPELINE_FILE) # We are just pickling it locally first\n\ntransformed_diamonds_df = preprocessing_pipeline.fit(diamonds_df).transform(diamonds_df)\ntransformed_diamonds_df"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "160f2c47-584a-4270-9c53-0a401eccc720",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "resultHeight": 354
      },
      "outputs": [],
      "source": "# You can also save the pickled object into the stage we created earlier for deployment\nsession.file.put(PIPELINE_FILE, \"@ML_HOL_ASSETS\", overwrite=True)"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "d4bd6167-3f3f-4236-a962-fd5fd019bc12",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "resultHeight": 155,
        "codeCollapsed": true
      },
      "source": "### Data Exploration\n\nNow that we've transformed our features, let's calculate the correlation using Snowflake ML's `correlation()` function between each pair to better understand their relationships.\n\n*Note: Snowflake ML's pearson correlation function returns a Pandas DataFrame*"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b5fb644-2cf4-4721-82e4-311465d14ad2",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "resultHeight": 0
      },
      "outputs": [],
      "source": "corr_diamonds_df = correlation(df=transformed_diamonds_df)\ncorr_diamonds_df # This is a Pandas DataFrame"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8d8c572-5119-4288-9d32-c42f157aaf37",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "resultHeight": 0
      },
      "outputs": [],
      "source": "# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr_diamonds_df, dtype=bool))\n\n# Create a heatmap with the features\nplt.figure(figsize=(7, 7))\nheatmap = sns.heatmap(corr_diamonds_df, mask=mask, cmap=\"YlGnBu\", annot=True, vmin=-1, vmax=1)"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ba002b6c-9819-46cf-8b6a-7ff589150e49",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "resultHeight": 109,
        "codeCollapsed": true
      },
      "source": "We note that `CARAT` and `PRICE` are highly correlated, which makes sense! Let's take a look at their relationship a bit closer.\n\n*Note: You will have to convert your Snowpark DF to a Pandas DF in order to use matplotlib & seaborn.*"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cb10563-3acc-4db4-98fa-4e5b0dc9d553",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "resultHeight": 1473
      },
      "outputs": [],
      "source": "# Set up a plot to look at CARAT and PRICE\ncounts = transformed_diamonds_df.to_pandas().groupby(['PRICE', 'CARAT', 'CLARITY_OE']).size().reset_index(name='Count')\n\nfig, ax = plt.subplots(figsize=(20, 20))\nplt.title('Price vs Carat', fontsize=28)\nax = sns.scatterplot(data=counts, x='CARAT', y='PRICE', size='Count', hue='CLARITY_OE', markers='o')\nax.grid(axis='y')\n\n# The relationship is not linear - it appears exponential which makes sense given the rarity of the large diamonds\nsns.move_legend(ax, \"upper left\")\nsns.despine(left=True, bottom=True)"
    },
    {
      "cell_type": "markdown",
      "id": "a29501c1-3653-4b51-81bb-439c35383db7",
      "metadata": {
        "collapsed": false,
        "resultHeight": 67,
        "codeCollapsed": true
      },
      "source": "In the next notebook, we will look at how you can train an XGBoost model with the diamonds dataset."
    }
  ]
}