{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "1f4adaa3-8ef6-4c8f-8057-1d12bf900962",
      "metadata": {
        "collapsed": false,
        "resultHeight": 179,
        "codeCollapsed": true
      },
      "source": [
        "## 1. Data Ingestion\n",
        "\n",
        "The `diamonds` dataset has been widely used in data science and machine learning. We will use it to demonstrate Snowflake's native data science transformers in terms of database functionality and Spark & Pandas comportablity, using non-synthetic and statistically appropriate data that is well known to the ML community.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "6fe2a4b4-821f-410c-8d2c-e527829b106e",
      "metadata": {
        "resultHeight": 46,
        "codeCollapsed": true
      },
      "source": [
        "### Import Libraries\n",
        "\n",
        "We'll use pandas to read the local CSV file and then convert it to a Snowpark DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7eed4494-06ab-43b2-86e8-879c99c1a2c0",
      "metadata": {
        "language": "python",
        "resultHeight": 0
      },
      "outputs": [],
      "source": [
        "# Snowpark for Python\n",
        "from snowflake.snowpark.types import DoubleType\n",
        "import snowflake.snowpark.functions as F\n",
        "from snowflake.snowpark.context import get_active_session\n",
        "\n",
        "# Pandas for reading local CSV\n",
        "import pandas as pd"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "39326625-0d17-438f-a847-8d1e2c1488da",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "resultHeight": 113,
        "codeCollapsed": true
      },
      "source": [
        "### Setup and establish Secure Connection to Snowflake\n",
        "\n",
        "Notebooks establish a Snowpark Session when the notebook is attached to the kernel. We create a new warehouse, database, and schema that will be used throughout this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76605036-bbf0-45cd-ac16-8c7bb391384c",
      "metadata": {
        "language": "sql",
        "resultHeight": 87,
        "resultVariableName": "init_sql"
      },
      "outputs": [],
      "source": [
        "-- Using Warehouse, Database, and Schema created during Setup\n",
        "USE WAREHOUSE ML_HOL_WH;\n",
        "USE DATABASE ML_HOL_DB;\n",
        "USE SCHEMA ML_HOL_SCHEMA;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "046fa3ea-5ea9-4af6-a4dd-88c7101dcf0d",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "resultHeight": 150
      },
      "outputs": [],
      "source": [
        "# Get Snowflake Session object\n",
        "session = get_active_session()\n",
        "session.sql_simplifier_enabled = True\n",
        "\n",
        "# Add a query tag to the session.\n",
        "session.query_tag = {\"origin\":\"sf_sit-is\", \n",
        "                     \"name\":\"e2e_ml_snowparkpython\", \n",
        "                     \"version\":{\"major\":1, \"minor\":0,},\n",
        "                     \"attributes\":{\"is_quickstart\":1}}\n",
        "\n",
        "# Current Environment Details\n",
        "print('Connection Established with the following parameters:')\n",
        "print('User      : {}'.format(session.get_current_user()))\n",
        "print('Role      : {}'.format(session.get_current_role()))\n",
        "print('Database  : {}'.format(session.get_current_database()))\n",
        "print('Schema    : {}'.format(session.get_current_schema()))\n",
        "print('Warehouse : {}'.format(session.get_current_warehouse()))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e9f22ce6-da50-4d76-98e1-3ff6246ed220",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "resultHeight": 153,
        "codeCollapsed": true
      },
      "source": [
        "### Read the local diamonds CSV file with pandas and convert to Snowpark DataFrame\n",
        "\n",
        "We'll read the `diamonds.csv` file from the local `data/` folder using pandas, then convert it to a Snowpark DataFrame for processing in Snowflake.\n",
        "\n",
        "For more information on creating DataFrames, see documentation on [snowflake.snowpark.Session.create_dataframe](https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/latest/api/snowflake.snowpark.Session.create_dataframe).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6080dc92-595e-48b6-b4fc-667e2346bed9",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "resultHeight": 439
      },
      "outputs": [],
      "source": [
        "# Read the local CSV file with pandas\n",
        "diamonds_pd = pd.read_csv('../data/diamonds.csv')\n",
        "\n",
        "# Convert pandas DataFrame to Snowpark DataFrame\n",
        "diamonds_df = session.create_dataframe(diamonds_pd)\n",
        "\n",
        "diamonds_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d04f72db-d8b4-4f25-aaf0-928475009895",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "resultHeight": 252
      },
      "outputs": [],
      "source": [
        "# Look at descriptive stats on the DataFrame\n",
        "diamonds_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8205f888-1a4a-4033-b4f4-ddbb1bc30e67",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "resultHeight": 360
      },
      "outputs": [],
      "source": [
        "diamonds_df.columns"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "8babcdbe-083d-4e4e-b7ec-ac86982e775d",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "resultHeight": 113,
        "codeCollapsed": true
      },
      "source": [
        "### Data cleaning\n",
        "\n",
        "First, let's force headers to uppercase using Snowpark DataFrame operations for standardization when columns are later written to a Snowflake table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcd0ce8f-0f7d-4cc9-ad07-f59d5c8e67a2",
      "metadata": {
        "language": "python",
        "resultHeight": 439
      },
      "outputs": [],
      "source": [
        "# Force headers to uppercase\n",
        "for colname in diamonds_df.columns:\n",
        "    if colname == '\"table\"':\n",
        "       new_colname = \"TABLE_PCT\"\n",
        "    else:\n",
        "        new_colname = str.upper(colname)\n",
        "    diamonds_df = diamonds_df.with_column_renamed(colname, new_colname)\n",
        "\n",
        "diamonds_df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "93794112-57e4-442e-b598-8c87aa31594e",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "resultHeight": 134,
        "codeCollapsed": true
      },
      "source": [
        "Next, we standardize the category formatting for `CUT` using Snowpark DataFrame operations.\n",
        "\n",
        "This way, when we write to a Snowflake table, there will be no inconsistencies in how the Snowpark DataFrame will read in the category names. Secondly, the feature transformations on categoricals will be easier to encode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b03328f0-f85a-4296-bfce-923a44aeadf7",
      "metadata": {
        "codeCollapsed": false,
        "language": "python"
      },
      "outputs": [],
      "source": [
        "def fix_values(columnn):\n",
        "    return F.upper(F.regexp_replace(F.col(columnn), '[^a-zA-Z0-9]+', '_'))\n",
        "\n",
        "for col in [\"CUT\"]:\n",
        "    diamonds_df = diamonds_df.with_column(col, fix_values(col))\n",
        "\n",
        "diamonds_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "654c8a95-271b-440b-a579-8f42d8f6f135",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "resultHeight": 41,
        "codeCollapsed": true
      },
      "source": [
        "Check the schema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "989ec169-24e9-477f-8170-a5c332249269",
      "metadata": {
        "codeCollapsed": false,
        "language": "python"
      },
      "outputs": [],
      "source": [
        "list(diamonds_df.schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a09273ca-c6a9-461f-8697-c703b62b986f",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "codeCollapsed": true
      },
      "source": [
        "Finally, let's cast the decimal types to DoubleType() since DecimalType() isn't support by Snowflake ML at the moment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "988261a8-7619-48c8-8241-6aa490e6d2d4",
      "metadata": {
        "codeCollapsed": false,
        "language": "python"
      },
      "outputs": [],
      "source": [
        "for colname in [\"CARAT\", \"X\", \"Y\", \"Z\", \"DEPTH\", \"TABLE_PCT\"]:\n",
        "    diamonds_df = diamonds_df.with_column(colname, diamonds_df[colname].cast(DoubleType()))\n",
        "\n",
        "diamonds_df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "54aea408-5f24-4fc1-9add-de83caed2b05",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "codeCollapsed": true
      },
      "source": [
        "### Write cleaned data to a Snowflake table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d00d0c5-157e-461b-b3e7-6577e081935d",
      "metadata": {
        "codeCollapsed": false,
        "language": "python"
      },
      "outputs": [],
      "source": [
        "diamonds_df.write.mode('overwrite').save_as_table('diamonds')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57a0fb14-608b-49c0-87e8-721c71cb5688",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "codeCollapsed": true
      },
      "source": [
        "In the next notebook, we will perform data transformations with the Snowflake ML Preprocessing API for feature engineering. "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "lastEditStatus": {
      "authorEmail": "sikha.das@snowflake.com",
      "authorId": "158808794318",
      "authorName": "SIKHADAS",
      "lastEditTime": 1747162667924,
      "notebookId": "fxuknqmrsyl4bsjuj36h",
      "sessionId": "85518b5f-1383-4ff2-b734-8bbee335fe2f"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}